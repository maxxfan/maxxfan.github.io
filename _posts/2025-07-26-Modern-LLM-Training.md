---
title: 'Modern Large Language Model Training: From First Principles to Advanced Paradigms'
date: 2025-07-26
permalink: /posts/2025/07/Modern-LLM-Training/
tags:
  - LLM
  - Training
---
Large Language Models (LLMs) have rapidly evolved from modest sequence predictors into enormous, general-purpose AI systems. Training these models involves next-token prediction on massive text corpora – essentially teaching the model to predict the next word given a history of words. This simple objective, when scaled up with billions of parameters and trillions of training tokens, produces models capable of astonishing conversational fluency, coding, reasoning, and more. In this report, we comprehensively explore modern LLM training: starting from first principles of language modeling and the Transformer architecture, and advancing through cutting-edge paradigms like mixture-of-experts scaling, ultra-long context handling, multi-modality, chain-of-thought reasoning, and agent-based learning. We will intermix theoretical foundations with practical implementation details – including mathematical formulations and pseudocode – to illuminate how today’s state-of-the-art models (e.g. Google Gemini 2.5, Alibaba’s Qwen 3, DeepSeek V3/R1, Moonshot’s Kimi K2) are trained and optimized. Throughout, we cite recent papers and technical reports (mostly 2024-2025) to ground our discussion in up-to-date research findings.

***Disclaimer: This post is generated by ChatGPT o3 Deep Research, with careful human instruction and verification.***

# Table of Contents
- [Introduction](#introduction)
- [Fundamentals of Language Modeling and Training Objectives](#fundamentals-of-language-modeling-and-training-objectives)
- [The Transformer Architecture: Theory and Practice](#the-transformer-architecture-theory-and-practice)
- [Scaling Up: From Millions to Trillions of Parameters](#scaling-up-from-millions-to-trillions-of-parameters)
- [Mixture-of-Experts (MoE) Models for Extreme Scale](#mixture-of-experts-moe-models-for-extreme-scale)
- [Extending Context Length: Long-Context and Memory Mechanisms](#extending-context-length-long-context-and-memory-mechanisms)
- [Multimodal Training: Incorporating Vision (and Beyond)](#multimodal-training-incorporating-vision-and-beyond)
- [Fine-Tuning and Alignment: From Pre-trained Model to Helpful Assistant](#fine-tuning-and-alignment-from-pre-trained-model-to-helpful-assistant)
- [Reasoning and Chain-of-Thought Training](#reasoning-and-chain-of-thought-training)
- [Agent-Based Training and Tool Use](#agent-based-training-and-tool-use)
- [Conclusion and Outlook](#conclusion-and-outlook)

# Introduction

Large Language Models (LLMs) have rapidly evolved from modest sequence predictors into enormous, general-purpose AI systems. Training these models involves **next-token prediction** on massive text corpora – essentially teaching the model to predict the next word given a history of words. This simple objective, when scaled up with billions of parameters and trillions of training tokens, produces models capable of astonishing conversational fluency, coding, reasoning, and more. In this report, we comprehensively explore **modern LLM training**: starting from first principles of language modeling and the [Transformer architecture](https://arxiv.org/abs/1706.03762), and advancing through cutting-edge paradigms like [mixture-of-experts (MoE)](https://arxiv.org/abs/2101.03961) scaling, ultra-long context handling, multi-modality, chain-of-thought reasoning, and agent-based learning. We will intermix theoretical foundations with practical implementation details – including mathematical formulations and pseudocode – to illuminate how today’s state-of-the-art models (e.g. [Google Gemini 2.5](https://blog.google/technology/ai/google-gemini-ai/), [Alibaba’s Qwen 3](https://github.com/QwenLM), [DeepSeek V3/R1](https://github.com/deepseek-ai/), [Moonshot’s Kimi K2](https://github.com/MoonshotAI/Kimi-K2)) are trained and optimized. Throughout, we cite recent papers and technical reports (mostly 2024–2025) to ground our discussion in up-to-date research findings.

The report is organized into chapters that progressively build from fundamentals to advanced topics. We begin by reviewing the basic training objective and the Transformer neural network that underpins virtually all modern LLMs. Next, we discuss how engineers scale up data, model size, and compute – obeying [scaling laws](https://arxiv.org/abs/2001.08361) and using distributed training infrastructure. We then delve into **mixture-of-experts (MoE)** architectures, a crucial technique enabling trillion-parameter models like DeepSeek V3 and Kimi K2 without commensurate increases in computation. After that, we examine strategies for **extending context windows** to hundreds of thousands or even millions of tokens, as seen in LLaMA 4 and Gemini 2.5, and how models cope with such long sequences. We also cover **multi-modal training** (combining text with images or other data) which makes models like LLaMA 4 and Gemini 2.5 “see” images or even video. Moving beyond pre-training, we explore **fine-tuning and alignment** methods – from supervised instruction tuning to [reinforcement learning from human feedback (RLHF)](https://arxiv.org/abs/2009.01325) – that align LLMs with human intentions and values. Finally, we focus on two cutting-edge paradigms: **explicit reasoning with chain-of-thought**, where models are trained to perform intermediate reasoning steps (a hallmark of “thinking” models like Gemini 2.5 and DeepSeek R1), and **agentic training**, where models learn to interact with tools or environments to solve tasks (exemplified by agent-oriented models like Kimi K2 and the tool-use optimizations in Qwen 3).

By the end of this report, the reader (targeted toward ML engineers and researchers) will have a deep understanding of how modern large language models are built and trained – from the ground up to the latest innovations. We include equations for key concepts (e.g. the attention mechanism, loss functions, policy gradients) and snippets of code/pseudocode illustrating implementation details of training loops, model components, or special features. Each chapter contains as much implementation insight as possible to bridge theory with practice. We also highlight specific real-world models as case studies to contextualize the concepts (with citations for further reading on those models). Let us begin our journey from first principles.

# Fundamentals of Language Modeling and Training Objectives

At its core, training a language model is about **learning to predict text**. Formally, given a sequence of tokens (words or subword pieces) $(w_1, w_2, \dots, w_N)$, an LLM is trained to estimate the probability distribution of the next token $w_{t+1}$ given all previous tokens $w_1 \dots w_t$. The training objective is usually to **maximize the likelihood** of the training data under the model, which is equivalent to minimizing the cross-entropy loss between the model’s predicted distribution and the true next token. If the model’s parameters are $\theta$, the loss for one token prediction can be written as:

$$
L(\theta) = -\log P_\theta(w_{t+1} \mid w_1, \ldots, w_t)
$$

Over an entire sequence, the model’s loss (negative log-likelihood) is the sum of per-token losses. In practice, we average over all tokens in a batch. If the model outputs a probability $P_\theta(v \mid \text{context})$ for each token $v$ in the vocabulary, and $y$ is the index of the true next token, the cross-entropy loss is:

$$
L(\theta) = -\frac{1}{B} \sum_{i=1}^{B} \log P_\theta(y_i \mid \text{context}_i)
$$

where $B$ is the batch size. This loss is differentiable, so we can compute gradients $\nabla_\theta L$ and update the parameters via gradient descent (typically using variants like **Adam** optimizer).

**Teacher Forcing**: During training, the true previous tokens are supplied as input (a technique known as teacher forcing). The model learns by comparing its predicted next-token distribution to the actual next token. The softmax cross-entropy loss penalizes the model if it assigns low probability to the correct token. Over many epochs and an enormous corpus, this encourages the network to build an internal representation of language that captures statistical patterns, syntax, semantics, and even factual/world knowledge. By the end of training, the model can be used to generate text by sampling one token at a time from $P_\theta(\cdot \mid \text{history})$ and feeding its own outputs back in as context.

**Perplexity**: A common metric to evaluate a language model’s quality during training is perplexity, which is simply $2^{\text{cross-entropy}}$. Lower perplexity indicates the model is assigning higher probability to the observed text (i.e. it predicts more confidently/correctly). As models train on more data and get larger, perplexity on held-out text typically decreases. Modern large models achieve very low perplexities on diverse corpora, reflecting their predictive power.

**Example Implementation – Training Loop**: The pseudocode below sketches a simplified training loop for a language model using PyTorch-like syntax, highlighting how data flows and gradients update the model:

```python
model = TransformerLM(vocab_size, num_layers, hidden_dim, ...)  # initialize model
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

for batch in dataloader:  # iterate over dataset
    inputs, targets = batch  # 'inputs' is a batch of token sequences, 'targets' is next-token labels
    logits = model(inputs)            # forward pass: logits for each sequence position
    # Compute cross-entropy loss between logits and targets
    loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))
    optimizer.zero_grad()
    loss.backward()                   # backpropagate to compute gradients
    optimizer.step()                  # update model parameters
```
In practice, many additional details are involved (learning rate schedules, gradient clipping, mixed precision, distributed data parallelism, etc.), but this snippet conveys the basic idea: predict next tokens and use gradient descent to make those predictions more accurate over time.

**Data and Tokenization**: The training data for LLMs is typically a vast collection of text (e.g. web pages, books, code, Wikipedia). Before training, text is converted to tokens (often using a Byte-Pair Encoding or similar subword tokenizer). The sequence length (context window) used during training might be fixed (e.g. 2048 tokens for many GPT-3 class models) or gradually increased during training for models that will support longer contexts. For example, the Qwen-3 model from Alibaba was trained in stages: first on 4K token sequences, then on longer sequences up to 32K to extend its context handling. The position of each token in the sequence is also encoded (traditionally via positional embeddings or newer techniques discussed later). The model then processes these token embeddings through its layers to produce next-token predictions.

Modern LLMs scale on tokens, not just parameters. Recent tech reports disclose substantially larger and more curated corpora with explicit domain balancing (code, math/STEM, multilingual text) and long‑context curricula. 
- Llama 4 (Meta). The released 17B MoE variants were pretrained on multimodal corpora: Llama 4 Scout on ~40T tokens and Llama 4 Maverick on ~22T tokens. The mixture combines publicly available, licensed data and information from Meta’s products and services (including publicly shared Instagram/Facebook posts and people’s interactions with Meta AI). Pretraining cutoff: August 2024. Although only 12 languages are “supported” in the model card, the pretraining corpus itself spans ~200 languages. 
- Qwen 3 (Alibaba). Qwen 3 reports ~36T pretraining tokens across 119 languages/dialects, built in three stages: (S1) ~30T tokens at 4,096 context for general knowledge; (S2) ~5T knowledge‑intensive tokens upweighting STEM, coding, reasoning, and synthetic data; (S3) long‑context training at 32,768 tokens using hundreds of billions of tokens with length distribution 75% in 16,384–32,768 and 25% in 4,096–16,384. Data expansion used Qwen2.5‑VL to OCR large PDF‑like documents and domain models (Qwen2.5‑Math/Coder) to synthesize trillions of additional tokens. Qwen introduced a multilingual data annotation system labeling 30T+ tokens for educational value, field, domain, and safety to optimize mixtures at the instance level. Tokenizer: BBPE, 151,669 vocab. 
- DeepSeek‑V3. Pretrained on 14.8T tokens with an explicit increase in math and programming ratios and broader multilingual coverage (beyond English/Chinese). The pipeline applies document packing, Fill‑in‑Middle (FIM) with a Prefix–Suffix–Middle schema at 10% rate, and a Byte‑level BPE tokenizer with 128K vocab. Long‑context is extended post‑pretraining from 4K → 32K → 128K using YaRN. 
- Kimi K2 (Moonshot). A 1T‑parameter MoE model (32B activated) pretrained on ~15.5T tokens, with context length 128K and 160K vocabulary; public materials emphasize agentic/tool‑use orientation. 
What this means for your pipeline? Category balance matters. Recent mixtures explicitly upweight code, math/STEM, and reasoning (Qwen 3; DeepSeek‑V3) and use synthetic domain data at trillion‑token scale to densify scarce skills. Multilingual scaling is now the norm. Corpora span 100+ languages (Llama 4, Qwen 3), with dedicated annotation/selection to control quality and safety across languages. Long‑context is trained, not just extrapolated. Teams use staged schedules and targeted long‑document corpora (e.g., Qwen 3’s 75% of samples ≥16k tokens; DeepSeek’s YaRN extensions). Modern tokenizers are large and byte‑level. Expect 128K–160K vocabularies (DeepSeek‑V3 128K; Kimi K2 160K; Qwen 3 151,669) to compress multilingual/code‑heavy text effectively. 

In summary, the first principles of LLM training rest on a simple maximum-likelihood objective applied at a very large scale. By predicting missing words, the model gradually learns a rich statistical representation of language. The next section describes the neural network architecture – the Transformer – that allows learning these representations effectively for long sequences.

# The Transformer Architecture: Theory and Practice

Modern large language models all rely on the **Transformer** architecture, a neural network design introduced by Vaswani et al. in 2017 (the famous “[Attention Is All You Need](https://arxiv.org/abs/1706.03762)” paper). The Transformer is particularly suited for language modeling because it can attend to (i.e. focus on) any part of the input sequence, enabling it to capture long-range dependencies and complex patterns in text. Here we review the key components of Transformers and relevant mathematics, then discuss implementation details for large models.

**Self-Attention Mechanism**: The core innovation of Transformers is the self-attention mechanism. In a self-attention layer, each token in the input sequence weighs the importance of every other token to itself in order to compute a new representation. Each token $i$ computes a weighted sum of the **values** of all tokens $j$, where the weight comes from a similarity between a **query** vector for token $i$ and a **key** vector for token $j$. In formula form, if $Q$, $K$, $V$ are matrices containing the query, key, and value vectors for all tokens (each row corresponds to a token position):

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right) V
$$

Here $d$ is the dimensionality of the query/key vectors (we divide by $\sqrt{d}$ for scaling stability as in Vaswani et al.), and the softmax is applied row-wise to produce attention weights that sum to 1 across the sequence. This operation yields an output of the same shape as $Q$ (one output vector per token), which is a blend of the value vectors $V$ with weights determined by how well each other token’s key matches the token’s query.

Transformers use **multi-head attention**, meaning the mechanism is replicated $h$ times with different learned linear projections for $Q, K, V$ (so each head can attend to different aspects). The outputs of all heads are concatenated and linearly transformed to form the final attention output. Multi-head attention allows the model to simultaneously consider various types of relationships (e.g. syntax, coreference, semantic similarity) at different positions.

**Feed-Forward Network and Layer Structure**: In addition to the attention sublayer, each Transformer block has a position-wise feed-forward network (FFN). This is typically a two-layer MLP applied independently to each token’s representation, increasing the model’s capacity to transform and non-linearly mix features. A common design is an FFN with an intermediate dimension of 4× the model’s hidden size and a GeLU or SwiGLU activation. The output of the FFN is added (with a residual skip connection) to the attention output. Layer normalization is applied at appropriate points (often just before attention and FFN, or as a combined pre-norm or post-norm style).

To summarize a single Transformer layer in pseudocode:
```python
class TransformerBlock(nn.Module):
    def __init__(self, hidden_dim, num_heads, ffn_dim):
        self.self_attn = nn.MultiheadAttention(hidden_dim, num_heads)
        self.layernorm1 = nn.LayerNorm(hidden_dim)
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, ffn_dim),
            nn.GELU(),
            nn.Linear(ffn_dim, hidden_dim)
        )
        self.layernorm2 = nn.LayerNorm(hidden_dim)
    def forward(self, x, attn_mask=None):
        # Self-attention sublayer
        x_norm = self.layernorm1(x)
        attn_output, _ = self.self_attn(x_norm, x_norm, x_norm, attn_mask=attn_mask)
        x = x + attn_output        # add residual connection
        # Feed-forward sublayer
        x_norm = self.layernorm2(x)
        ffn_output = self.ffn(x_norm)
        x = x + ffn_output         # add residual connection
        return x
```

This simplistic code highlights the components: multi-head attention and feed-forward network with skip connections. In practice, modern architectures introduce variations (different normalization placements, gating mechanisms, etc.), but the essence remains: alternating attention and pointwise FFN layers.

**Causal Masking**: For language generation, the Transformer is used in decoder-only mode with causal self-attention. This means each token can only attend to earlier tokens, not future ones, to prevent information leakage from the future during training. A triangular attention mask is applied so that for position $i$, queries only “see” keys $j \le i$. This enforces the autoregressive property needed for proper language modeling. During inference, the model generates one token at a time and extends the context.

**Positional Encodings**: Since the Transformer has no built-in notion of token order (self-attention is permutation-invariant except for how queries/keys might implicitly learn order), we inject positional information. Originally, sinusoidal positional encodings were added to token embeddings. Newer models often use **[rotary position embeddings (RoPE)](https://arxiv.org/abs/2104.09864)** or other schemes that allow better generalization to longer sequences than seen in training. RoPE multiplies query and key vectors by a rotation matrix that depends on position, effectively encoding relative phase that the attention mechanism can use to infer order. Many 2023+ LLMs (e.g. LLaMA series, Mistral, etc.) use RoPE to handle extended context lengths.

**Model Size and Depth**: The hidden dimension (model width) and number of layers determine the parameter count along with vocabulary size. For example, a model like LLaMA 2 70B had 80 layers and a hidden size of 4096. Modern frontier models are even larger – e.g. Llama 4 uses a mixture-of-experts but each expert path is akin to a ~17B parameter model, and Kimi K2 has 61 layers with a 7168-wide hidden size and 64 attention heads. As models scale, certain architectural choices change (e.g. using sparse attention patterns, or adding specialized layers for stability). Nonetheless, the fundamental Transformer block remains the workhorse.

**Training the Transformer**: During training, a sequence of tokens is passed through a stack of Transformer blocks. The output of the final block goes into a linear layer that projects to vocabulary logits. This yields $P_\theta(\text{next token}\mid \text{context})$ after softmax. The cross-entropy loss as described earlier is computed against the true next token. Then backpropagation updates all parameters in the embedding matrix, each Transformer layer, and the output projection. Training is extremely computationally intensive for large models – it requires millions of iterations and a distributed computing setup (discussed in the next section).

Despite the complexity of Transformers, one can view them conceptually as implementing an adaptive, content-driven feature extractor on sequences. Early layers may detect low-level patterns (phrases, local syntax), while deeper layers aggregate information globally (paragraph or document-level coherence, world knowledge). The attention mechanism allows dynamic weighting of relevant context for each prediction, enabling handling of long texts with relevant dependencies potentially far apart in the sequence.

In implementation, frameworks like PyTorch and TensorFlow offer high-level APIs for multi-head attention and Transformer layers, which are heavily optimized (especially on GPU/TPU hardware). Nonetheless, training a 100B+ parameter Transformer from scratch pushes these frameworks to their limits, requiring special optimizations which we will touch on.

Having established the architecture and base training objective, we can now discuss how one scales up these models to modern gargantuan sizes and what strategies are used to efficiently train them.

# Scaling Up: From Millions to Trillions of Parameters

Early language models had on the order of millions or low billions of parameters. Modern LLMs have exploded in scale – **tens or hundreds of billions** of parameters are routine, and some exceed a trillion. Alongside model size, the amount of training data (tokens) and compute budget have also grown exponentially. This section examines the principles of scaling, including scaling laws, and the engineering techniques to train these giant models.

**Scaling Laws**: Pioneering work by Kaplan et al. (2020) observed power-law relationships between model size, dataset size, and performance. In short, larger models (more parameters $N$) and more training data ($D$ tokens) yield lower loss, with diminishing returns in each direction. A key finding was that for a given model size, there’s an optimal amount of data; training much beyond that yields overfitting or inefficiency, whereas too little data under-utilizes the capacity. In 2022, DeepMind’s [Chinchilla report](https://arxiv.org/abs/2212.14034) refined this: they found that for transformers, optimal $D$ scales roughly linearly with $N$ (specifically, $D \propto 20 N$ was suggested for best use of compute). This insight led to smaller models trained on more data outperforming some larger-but-undertrained models.

As a result, modern training runs ensure sufficient data is fed. For example, Meta’s Llama 4 models were trained on trillions of tokens: the 17B-parameter Llama 4 “Scout” saw about **40 trillion tokens** during pretraining, while the 17B×128 experts “Maverick” saw ~22T (likely because its effective capacity is larger so fewer passes suffice). Similarly, Alibaba’s Qwen 3 was trained on **36 trillion tokens**, roughly double the data of its predecessor, to fully leverage its larger size. These numbers are orders of magnitude above what was used for GPT-3 (which used 300 billion tokens) or even GPT-4 (OpenAI hasn’t disclosed, but estimates often put it in low trillions). Clearly, respecting scaling laws has pushed data requirements to new heights.

On the performance side, scaling laws imply that as you increase model size $N$, the test loss tends to follow a power-law decay until bottlenecked by data. In practice, bigger models almost always perform better on a broad range of tasks (provided they are adequately trained). For example, the Qwen3 blog notes that their small 4B model can rival older 72B models by benefit of advanced training, and the large Qwen3-235B matches the very best models like DeepSeek R1 or OpenAI’s latest on various benchmarks. DeepSeek-V3 (37B activated params via MoE) managed to match or outperform many closed models while using only ~2.8 million GPU hours for training, demonstrating efficient scaling.

**Compute and Hardware**: Scaling up is limited by available compute. Training a 100B+ parameter model on trillions of tokens requires an enormous number of floating-point operations (FLOPs). For perspective, training LLaMA 4 Scout (17B×16 experts) took **5.0 million GPU-hours on Nvidia H100s**, and Maverick took 2.38M, for a total of 7.38M GPU-hours. Assuming an H100 can do on the order of $10^{14}$ FLOPs per second, we’re talking $10^{23}$ FLOPs of training work (hundreds of zettaflops). Only large data centers with hundreds or thousands of GPUs can handle this. Companies use optimized clusters – for example, Meta’s custom GPU cluster and infrastructure was used for LLaMA 4, and Google surely trained Gemini on their TPU v4 pods or similar. Distributed training strategies are essential: data parallelism (sharding batches across GPUs), model parallelism (sharding the model layers or parameters across GPUs), and pipeline parallelism (pipelining different micro-batches across layers on different GPUs) are combined (often via libraries like [DeepSpeed](https://www.deepspeed.ai/), [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) or [JAX pmap](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)). Modern efforts often use [FSDP](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) (Fully Sharded Data Parallel) which shards model weights and optimizer states across workers to reduce memory, combined with gradient checkpointing to trade compute for memory. Without these, a trillion-parameter model simply wouldn’t fit in memory or be trainable.

**Mixed Precision**: A crucial implementation detail at scale is using lower precision arithmetic to speed up training and save memory. Most LLM training now uses mixed precision, typically [bfloat16](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format) or float16 for forward/backward passes, while keeping a copy of weights in higher precision (FP32) for stability. Gradient values are also often accumulated in FP32 to avoid precision loss. Newer developments include using FP8 formats – in fact, DeepSeek-V3 **for the first time validated the feasibility of FP8 training on an extremely large-scale model**. They used an FP8 mixed-precision framework and achieved stable training of their 671B total parameter MoE model. The benefit of FP8 is further memory and compute savings (allowing larger batch sizes or models on the same hardware). Nvidia’s H100 GPUs specifically added hardware support for FP8, and 2024-2025 models are beginning to adopt it for efficiency.

**Optimization and Learning Rate Schedules**: Large-scale training also requires careful tuning of the optimizer and learning rate schedule. [AdamW](https://arxiv.org/abs/1711.05101) (Adam with weight decay) remains a popular choice for LLMs. The learning rate is usually warmed up from 0 to a peak over a few thousand steps at the start (to avoid instability), then decayed (often with a cosine schedule or linear schedule) over the course of training. The total number of training steps can be in the millions. Large models are sensitive to optimization hyperparameters; even a slight mis-setting can cause divergence (loss blowing up). This has led to custom optimizer variants for stability. For example, Moonshot’s Kimi K2 uses a **[Muon optimizer](https://arxiv.org/abs/2502.16982)** with a technique called MuonClip to resolve instabilities when scaling up. Although details of MuonClip are beyond our scope, Kimi’s team noted they applied Muon at unprecedented scale (1 trillion parameters) and had to develop novel techniques to keep training stable. The result was they could train the 1T-parameter model on **15.5 trillion tokens with zero instabilities** (no loss spikes or blow-ups) – an impressive feat given how delicate such runs can be.

**Batch Size and Sequence Length**: As we scale, choosing batch size (number of sequences per iteration) and sequence length becomes constrained by hardware memory. Many large runs use very large batch sizes by aggregating across many GPUs – effective batch sizes of 0.5 to 1 million tokens per update are not uncommon. This improves training stability and speed (via better GPU utilization) but can require adjusting optimizer hyperparameters (too large a batch can converge poorly without learning rate tweaks). Sequence length used during training might also be increased over time (like Qwen3’s staged approach from 4K to 32K tokens). Some training regimens mix sequence lengths in each batch (to provide some shorter and some longer examples) to improve stability and efficiency of context usage.

**Domain balancing and Curriculum**: With trillion-token datasets, ensuring a good mix of content is crucial. Data often comes from multiple sources (web text, books, code, conversations, etc.). Modern LLM training pays attention to curation – for instance, de-duplicating data to avoid overfitting common texts, filtering out low-quality content, and balancing domains. Sometimes, a curriculum is used (though not always): e.g. start training on easier or more generic data and later introduce more specialized or difficult data. However, many large-scale runs simply shuffle a giant corpus. There is also a trend of using synthetic data generated by earlier models to enhance training (for domains like math or coding where human-written data is scarce). Qwen3 did this: they used their prior smaller models [Qwen2.5-Math and Qwen2.5-Coder](https://github.com/QwenLM/) to generate additional math problems, Q&A pairs, and code snippets, augmenting the training set. This gave them a boost in those areas without manual data collection. It’s a form of bootstrapping: using an existing model to create more training data for the next model.

In summary, scaling an LLM is a balancing act of model size $N$, data $D$, and compute. The high-level guiding principle is to make the model as large as feasible while providing it enough data to not underfit and enough compute budget to process that data. Adhering to scaling laws and leveraging optimized training techniques (distributed training, mixed precision, etc.) has allowed the field to push from the 100 million parameter models of 2017 to the trillion-parameter class models of today. Next, we turn to a specific architectural strategy that has been crucial for enabling trillion-parameter models without proportionally increasing computation: Mixture-of-Experts.

# Mixture-of-Experts (MoE) Models for Extreme Scale

One of the most important advancements in scaling LLMs is the **Mixture-of-Experts (MoE)** architecture. In a standard Transformer, every token passes through the same set of parameters in each layer. MoE, by contrast, introduces multiple parallel experts (sub-networks) at certain layers and a learned gating mechanism that routes each token through only a subset of those experts. This allows the model to have a vastly larger number of total parameters, while each token only activates a fraction of them – hence the computational cost per token stays manageable. The MoE approach was pioneered by Shazeer et al. (2017) and further popularized by Google’s Switch Transformer (2021), and it has seen a resurgence in 2023-2025 as researchers push model sizes to the trillion+ range.

**How MoE Works**: In an MoE layer (usually replacing or augmenting the feed-forward network of a Transformer block), there are $E$ expert feed-forward networks instead of one. A gating network (typically a lightweight linear layer on the token’s representation) produces a distribution over the $E$ experts for each token. Then, each token is dispatched to its top-$k$ experts (commonly $k=1$ or 2) for processing. The outputs from those experts (of shape [batch, hidden_dim]) are combined – often simply summed or weighted by the gate probabilities – to produce the token’s output for that layer. Importantly, because each token only goes to $k$ experts, the overall computation is sparse. The parameter count can be enormous (since you have $E$ experts each with substantial size), but at inference/training time each token only “sees” a fraction of them.

For a single token with input $h$, an MoE layer’s computation can be written as:

$$
\text{Gate}(h) = \text{softmax}(W_g h) \in \mathbb{R}^E
$$

where $W_g$ are gate weights. Let the top-$k$ experts chosen be indices $i_1, i_2, \ldots, i_k$, and $G_{i_j}$ be the gate’s probability for expert $i_j$. Each expert is a function $f_i(h)$ (like a feed-forward network). The MoE output is then:

$$
\text{MoE_output}(h) = \sum_{j=1}^{k} G_{ij}(h) \, f_{ij}(h)
$$

If $k=1$, this simplifies: the token is routed to a single expert with the highest gate score. Many implementations use $k=1$ for efficiency (this is the “Switch Transformer” case). Some use $k=2$ and then linearly combine the two expert outputs as above.

**Benefits**: By having many experts, the model can specialize different experts to different kinds of inputs. For instance, some experts might handle coding-related language, others handle everyday English, others handle different languages or factual domains. The gating mechanism can learn these allocations. The model’s capacity (total parameters) is $E$ times larger than a dense model of equivalent hidden size, but computational cost only grows with $k$ (usually small). This means you can scale to trillions of parameters without a trillion-level increase in FLOPs. It’s a way to overcome the otherwise quadratic growth of compute with model size.

For example, DeepSeek-V3 is described as “a strong Mixture-of-Experts language model with 671B total parameters with 37B activated for each token.”. This was achieved by having a large number of experts (their HuggingFace card mentions 37B activated per token, which implies perhaps 18 experts, since 37B × 18 ≈ 666B ~ 671B total). Indeed, they mention using a **[DeepSeekMoE](https://arxiv.org/abs/2405.20189) architecture** validated in V2, and they specifically **pioneered an auxiliary-loss-free strategy for load balancing**. One challenge in MoE training is ensuring that all experts get utilized (the gate can otherwise collapse to always choosing a few experts). Earlier MoE models added an auxiliary loss to encourage equal expert usage. DeepSeek-V3 claims to remove the need for that, presumably through a clever gating design that inherently balances load. The result is better performance without the penalty that aux losses can impose. They also introduced a **Multi-Token Prediction (MTP) objective** in pretraining that improved performance (predicting multiple tokens in one go, aiding efficiency).

Another example: Kimi K2 from Moonshot AI is a **1 trillion parameter MoE model with 32 billion activated parameters**. It uses 384 experts with 8 selected per token (as per their model card). This design (384 experts, top-8 routing) means on each token it aggregates outputs from 8 expert networks out of 384. Kimi’s key contribution was scaling MoE to the 1T scale stably. They report zero training instability even at that scale, crediting their optimizer innovations. The architecture of Kimi K2 shows just 1 dense layer among 61 total layers, meaning almost all layers are MoE except perhaps the first or last. With 384 experts in each MoE layer, the total params blow up to 1T, but thanks to MoE, the runtime cost is akin to a ~32B model (plus overhead for gating and communication). Kimi K2 achieves state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models – “non-thinking” here means it doesn’t internally do multi-step reasoning, which we’ll cover later. It essentially validates that a massive MoE model can rival the best dense models.

Meta’s Llama 4 series also adopted MoE – marking a new era where even a big player like Meta moved to sparse architectures for open models. Llama 4 launched Scout (17B with 16 experts) and Maverick (17B with 128 experts). In Scout, 16 experts of 17B each yields 109B total parameters, while Maverick’s 128 experts yield ~400B total. Each token in Maverick only activates 17B (presumably one expert’s worth), so its cost is like a dense 17B model. These Llama 4 models use an MoE variant and are natively multimodal (they handle images too, as discussed later). The adoption of MoE allowed them to offer much larger capacity within the memory limits of single GPU nodes: notably, they state Llama 4 Scout’s int4 quantized weights can fit on a single H100 (thanks to only 17B activated). They even provide code for efficient on-the-fly quantization to minimize performance loss.

**Training Challenges**: MoE does introduce complexity: - Load balancing – as mentioned, if the gate doesn’t use some experts, those experts effectively don’t learn and capacity is wasted. Solutions include adding entropy to the gating decisions, auxiliary losses, or setting gating thresholds. DeepSeek-V3’s elimination of aux loss suggests they found a more elegant balancing approach. - Communication overhead – in distributed training, if different tokens route to different experts on different GPUs, the system must send token data to the appropriate device. This can cause network bottlenecks. To mitigate this, efficient all-to-all communication algorithms and custom routing implementations are used. DeepSeek-V3 notes that through co-design of algorithms and hardware, they achieved near full computation-communication overlap in cross-node MoE training. This significantly boosted training efficiency, letting them scale further without extra cost. - Memory usage – storing a trillion parameters means that even if not all are active, they reside somewhere. Memory sharding across many GPUs is a must. Deployment is also tricky: you often need a system that only loads the needed experts per token (dynamic dispatch) to avoid loading the entire model into memory at once.

**Expert Specialization**: Empirically, researchers find that experts in an MoE do specialize to some extent. For instance, an expert might become very good at syntax, another at factual Q&A, etc., though this is not explicitly given to the model – it emerges from training. Alibaba’s Qwen3 has two MoE models: a large 235B-total (22B activated) and a smaller 30B-total (3B activated). They reported that Qwen3-30B-A3B (3B activated) outperforms a dense model Qwen2 with 10× active params, showing the efficiency of MoE. This indicates that those 3B active parameters, when distributed as experts, can punch above their weight due to specialization and greater total capacity behind them.

**Implementation**: Implementing MoE requires a routing algorithm. Frameworks like [Fairseq](https://github.com/facebookresearch/fairseq), [FastMoE](https://github.com/laekov/fastmoe), and the [Mixture-of-Experts TensorFlow](https://github.com/tensorflow/mesh) implementation provide primitives for this. Typically, one needs to: 1. Compute gating weights for each token. 2. Dispatch tokens to experts: e.g. create $E$ slices of the batch, send each token’s data to its selected expert’s slice. 3. Each expert processes its assigned tokens independently (these can run in parallel). 4. Combine outputs from experts back to the original token order.

Pseudocode for forward might look conceptually like:
```python
gates = gate_network(x)            # [batch, E] gate scores
topk_vals, topk_idx = topk(gates, k)
dispatch = create_dispatch_mask(topk_idx)  # binary mask [batch, E] indicating expert assignment
# Each expert processes its tokens
outputs = []

for i in range(E):
    x_i = x[dispatch[:, i]]        # tokens routed to expert i
    if x_i is not empty:
        y_i = expert_i(x_i)        # apply expert FFN
    else:
        y_i = None
    outputs.append(y_i)
# Combine outputs
y = combine_by_experts(outputs, topk_vals, topk_idx)
```
In practice, this is vectorized and optimized (for example, using a scatter/gather operation rather than explicit Python loops). Also, many toolkits use a fixed capacity per expert (if too many tokens choose the same expert beyond a threshold, some are dropped or re-routed to ensure load balance).

Modern large-scale MoEs often also use techniques like Expert dropout (dropping some experts during training to encourage redundancy) or switching routing strategies to improve robustness.

**Impact on Performance**: MoE models have achieved top-tier performance on many tasks. DeepSeek-V3 reports that it “outperforms other open-source models and achieves performance comparable to leading closed-source models”. In an evaluation cited on their model card, DeepSeek-V3 was very strong up to the maximum context of 128K tokens, presumably benefiting from both its MoE capacity and their long-context training. Kimi K2’s results show it beating or matching models like Anthropic’s [Claude 4](https://www.anthropic.com/index/claude-4) on many coding benchmarks when no chain-of-thought is used. The TechCrunch article noted Kimi K2 “matches or surpasses Western rivals, as well as some DeepSeek models, on various benchmarks”. It especially excels at coding tasks like [LiveCodeBench](https://huggingface.co/datasets/ise-uiuc/LiveCodeBench), likely because some experts and its large capacity are devoted to programming language modeling.

To conclude, mixture-of-experts is a powerful paradigm enabling scale without the full cost. By intelligently routing tokens, the model can be huge but use its capacity efficiently. The resurgence of MoE in 2024-2025 (Meta’s Llama, Alibaba’s Qwen, DeepSeek, Moonshot’s Kimi all embracing MoE) suggests it’s become a key tool for building the next generation of LLMs. As we move on, we’ll see that MoE often pairs with other innovations: for example, all these MoE models also support very long contexts. In the next section, we examine how models handle long context windows, which is another frontier in LLM capability.

# Extending Context Length: Long-Context and Memory Mechanisms

A traditional Transformer has a limitation: computational and memory cost grows quadratically with sequence length due to the $QK^T$ attention matrix. Early large models like GPT-3 used 2,048 token contexts. But many tasks benefit from much longer context – reading long documents, multi-turn dialogues, or storing a conversation’s full history. Thus, a major research thrust has been extending the context window of LLMs, finding ways to handle tens of thousands or even millions of tokens.

**Positional Encoding and Extrapolation**: One simple aspect is choosing a positional encoding that allows extrapolation. **Rotary Position Embeddings (RoPE)**, used in LLaMA and others, empirically allow models to generalize somewhat beyond the trained context length (e.g. a model trained on 2K can often handle 4K or more with degraded but not totally broken performance). Some open-source models exploited this by training with 4K and then using 8K or 16K at inference with reasonable success. However, to reliably handle very long inputs, models must be explicitly trained or architected for it.

**Efficient Attention Variants**: Numerous efficient attention mechanisms ([Linformer](https://arxiv.org/abs/2006.04768), [Performer](https://arxiv.org/abs/2009.14794), [Longformer](https://arxiv.org/abs/2004.05150), etc.) have been proposed to reduce the cost for long sequences. These often approximate the full attention or restrict the pattern (e.g. attention only to local neighbors plus a few global tokens). Longformer, for instance, uses a combination of local sliding window attention and global tokens to enable 8K+ sequences at less cost. However, most state-of-the-art LLMs have not adopted these approximate methods, instead opting for brute-force scaling with lots of memory or using hardware with large memory like TPUv4s. That said, Multi-Head Latent Attention (MLA) is something both DeepSeek and Kimi K2 mention. It’s possible MLA is a custom efficient attention approach that helps with long contexts, but details are scant in the public sources. Kimi’s card lists **Attention Mechanism: MLA**, implying they didn’t use standard scaled dot-product but a variant.

**Segmented Processing**: Another concept is to segment a long text and process it in chunks with some mechanism to pass information between chunks (like recurrence or an external memory). Transformer-XL (2019) introduced a recurrence mechanism that allows a transformer to reuse hidden states from previous segments, effectively achieving longer context without computing full attention over the entire sequence at once. This could, in theory, allow contexts of arbitrary length by sequentially processing segments. Some newer models or research prototypes use similar ideas. It’s not explicitly stated which new large models use recurrence, but given the extreme context claims (like millions of tokens), something beyond naive attention must be at play.

**LLaMA 4’s 10 Million Token Context**: Meta’s Llama 4 Scout made headlines with a 10 million token context window. This is unprecedented – 10 million tokens is roughly 7,500 pages of text. How is this even possible? Likely through a combination of approaches: - They did not attend over 10M tokens densely – that would be infeasible (attention matrix of 10M×10M). Instead, they might use a retrieval or chunking mechanism that effectively allows accessing 10M tokens of content. Some speculated that it could be a form of [Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2005.11401) built into the model (where it retrieves relevant pieces from a long context rather than attending to all). - The Llama 4 documentation suggests the 10M context is partly a “virtual” claim – “no model was trained on prompts longer than 256k tokens” meaning anything beyond 256k is uncharted territory and likely handled by extrapolation or external support. So Llama 4 Scout may have been trained on very long sequences (e.g. 128k or 256k tokens) and uses a position encoding that in principle extends to 10M. Perhaps they used a variant of [ALiBi](https://arxiv.org/abs/2108.12409) (Attention with Linear Bias) or other methods that allow extrapolation to extreme lengths. The F5 blog on RAG vs 10M context quotes an expert saying the >256k region might yield low-quality outputs. In other words, 10M is an upper bound if one were to try, but the model’s effective working context might be somewhat less. Nonetheless, even 256k is huge. - It’s also possible that Scout’s MoE architecture plays a role: maybe experts handle different segments of the context? If each expert had a limited receptive field, the combination might extend context. However, we don’t have confirmation of such an approach.

In practical evaluation, Llama 4 was tested up to 128k context on a “half-book” and “full-book” translation task, and it showed decent performance. This indicates it truly can make use of contexts in the hundreds of thousands of tokens. The Behemoth variant (apparently a preview model beyond Maverick, possibly 2 trillion parameters) likely also focuses on maximizing context, but little is published on it yet.

**Google Gemini 2.5’s Long Context**: Gemini 2.5 Pro ships with a 1 million token context window, with 2 million tokens coming soon. This suggests Google also solved or sidestepped some limitations. It’s likely they incorporate retrieval or hierarchical processing. They mention Gemini can handle entire code repositories and multiple sources like text, images, video – this implies both multi-modality and long context. Possibly, Gemini uses some form of hierarchical attention: maybe it can read chunks, summarize internally, and iteratively refine (this could be done either at inference via prompting or built-in via architecture). It might also involve an advanced memory management system where it doesn’t actively attend to all 1M tokens at every generation step, but can swap in relevant parts. There is research on such “expanding window” transformers or on-the-fly retrieval (e.g., the model could have been trained with an internal retrieval mechanism for long texts).

**Qwen-3 and Kimi K2 Context**: Qwen3’s context length is 128k for its larger models. They explicitly extended to 32k during training (stage 3), so 128k might be using RoPE extrapolation beyond 32k. Or they did a final fine-tune to 128k with high-quality long data. The TechCrunch notes that Qwen3 was tested on half and full book translation tasks, likely similar to Llama’s MTOB test, and presumably it performs reasonably up to 128k as well. Kimi K2 similarly advertises 128k context. It’s interesting that multiple leading open models settled on 128k as a feasible context length after training, hinting that beyond that might have diminishing returns or memory issues.

**Memory and Throughput Considerations**: Pushing context length strains GPU memory heavily (since attention matrix scales with square of length). For example, processing 100k tokens with a dense 32-head attention could require terabytes of memory if done naïvely. That’s why even if models support these lengths, in practice one might chunk the input or use model-specific optimized inference engines (like [vLLM](https://github.com/vllm-project/vllm), which can handle large contexts by offloading the key/value cache to CPU memory as needed). An analysis cited the KV cache for 10M tokens would be ~1.2 TB of memory for a model like Llama 4 – clearly impossible to keep on one machine, meaning any 10M context use would require streaming or retrieval rather than literal full context in memory.

**Retrieval-Augmented Generation (RAG)**: There’s an argument whether having extremely long context might obviate the need for retrieval (searching external knowledge) because the model can just take all relevant documents in context. The F5 blog mentioned there were hot takes about RAG becoming obsolete, but it defends that RAG is still needed. The reasoning is: feeding everything into context is inefficient and can pose security/privacy issues if you stuff sensitive data. RAG remains useful to pull only pertinent info on the fly. Likely, future models might combine both: a moderately large context plus an integrated retrieval mechanism for truly huge information sources.

**Techniques for Stability**: When extending context length during training, some tricks are needed. E.g., learning rate warmups or gradually increasing length can help the model adapt. Also, positional embeddings (like RoPE) might be scaled or pre-rotated differently. There are techniques like position interpolation (used by Meta in LLaMA 2 for 4k->32k fine-tune) which stretches learned positional embeddings to cover a longer range. If not done carefully, models can struggle with very far positions (they might forget earlier content or not know how to weight it).

DeepSeek R1 (the reasoning model) also had 128k context as it’s based on V3. They tested R1 on tasks requiring reasoning across long contexts and found strong performance across all lengths up to 128k. So clearly, focusing on stable long-range attention was part of their engineering.

To summarize, modern LLMs have blown past prior context limitations. Through a combination of: - Better positional encoding (RoPE, ALiBi, etc.) - Possibly sparsified attention or memory (though not always publicly detailed) - Training on some long sequences (to let the model learn to handle them) - And, in some cases, augmenting with retrieval or hierarchical processing,

we now have models that theoretically handle contexts from 100K to 1M or more tokens. In practice, extremely long contexts still face quality and speed challenges, but the trajectory is clear: context windows are expanding. This enables new applications like feeding entire books, massive logs, or codebases into the model for analysis.

One concrete example: Gemini 2.5 is said to achieve state-of-the-art 18.8% on “[Humanity’s Last Exam](https://arxiv.org/abs/2405.18366)” without tool use, across models without external tools. This dataset is designed to test frontier knowledge and reasoning, which presumably requires integrating lots of context from various domains. The long context and reasoning capabilities likely helped it achieve that.

Now that we’ve seen how models can read more and remember more, let’s look at another dimension of “more than just text”: multi-modality. Many modern LLMs are trained to handle not only text, but images and other modalities, which is our next topic.

# Multimodal Training: Incorporating Vision (and Beyond)

The ability to process and generate multiple modalities – such as images, audio, or video – is a significant advancement in LLM training. A “pure” large language model deals with text tokens only, but the world’s information isn’t just text. By training models on image-text pairs, audio transcripts, etc., researchers aim to create **multimodal models** that can see and talk (and maybe hear or act).

**Approaches to Multimodality**: There are two primary approaches: 1. **Early Fusion**: Integrate modalities at the input level. For example, represent an image as a sequence of visual tokens (via a vision encoder or patch embeddings) and prepend/insert those tokens into the text sequence. The model’s first layer can then attend across both image tokens and text tokens. This is the approach Meta’s Llama 4 uses – they mention “early fusion for native multimodality”, meaning the model directly takes image pixels (after some linear projection) alongside text. The model then is trained on tasks like describing images or answering questions about images, so it learns to align visual features with text. 2. **Late Fusion / Dual Encoders with Coordination**: Another design is to have a dedicated vision module whose outputs are then fed into the language model at a later stage (e.g., CLIP-like encoders feeding into the transformer). But for large models, the trend is toward a unified architecture if possible, since it allows full interaction between modalities.

**Llama 4’s Multimodality**: Llama 4 models are explicitly **“natively multimodal AI models”** enabling text and image understanding. They were optimized for vision tasks like image captioning, visual reasoning, etc., in addition to text and code. According to Meta’s card, Llama 4 can take up to 5 images as input in its vision-supporting fine-tuned version. If more images are given beyond 5, they caution the developer needs to test carefully. The architecture likely uses a learned projection to embed image patches or a CNN backbone frozen from something like [ViT](https://arxiv.org/abs/2010.11929) (though not specified, the early fusion implies perhaps using a vision transformer portion to get patch embeddings, then feeding those into the LLM). The result is a single model that can, for instance, interpret a photo and answer questions about it (like “What is happening in this image?”).

Performance-wise, Llama 4 matched or exceeded previous multimodal models. The [SSRN paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4813554) on Llama 4 (Ajit Singh, 2025) highlights an accuracy of 92% and F1 0.89 on multimodal benchmarks like [COCO](https://cocodataset.org/) image captioning and [Flickr30k](http://shannon.cs.illinois.edu/DenotationGraph/). This indicates Llama 4’s visual understanding is quite advanced. They also note it integrates audio, though Meta’s own materials only mention text & image. Perhaps audio was an experimental extension or the SSRN is speculating future capability. In any case, Llama 4 is positioned as a multimodal leap, even including mixture-of-experts and huge context (truly a convergence of many advanced features).

**Google Gemini’s Multimodality:** Google’s Gemini (even 2.0) was intended from the start to be multimodal (combining strengths of their text model PaLM and image model ViT/[Imagen](https://imagen.research.google/)). Gemini 2.5 explicitly “builds on what makes Gemini models great — native multimodality and a long context window.”. They mention 2.5 Pro can handle text, audio, images, video as input and output text. Likely, Gemini’s architecture includes specialized encoders or a joint embedding space. It’s possible they feed non-text modalities through encoders into the same transformer or have a unified model that was trained on multiple types of data. For example, audio could be converted to a sequence of embeddings (via a speech recognition model’s intermediate representation) and then concatenated to text tokens. Video could be treated as a sequence of image frames or frame features. The ability to accept entire videos and long audio suggests a lot of training on such data. It’s worth noting Google has a history here: e.g. their [Flamingo model](https://arxiv.org/abs/2204.14198) (DeepMind, 2022) was a precursor that allowed few-shot image+text via a gated cross-attention. Gemini might have integrated these ideas into the base model.

In evaluations, multimodal capability is often measured by tasks like visual question answering (e.g., Science QA with diagrams, or complex reasoning on images) and the model’s helpfulness in a dialogue setting with images. While we don’t have specific numbers for Gemini 2.5’s vision performance in the excerpt, Google has heavily emphasized it. A related blog (Jul 2025) on Gemini mentioned “Gemini’s multimodal capabilities” and even a podcast about it. That implies by mid-2025, Gemini can likely do tasks like describe an image, interpret a chart, use an image in reasoning (e.g., “Given this plot, what does it imply?”).

**Other Models:** Alibaba’s Qwen series had a variant Qwen-VL (vision-language) for image understanding. Qwen3 likely continues that capability, though the TechCrunch piece on Qwen3 focused on reasoning and MoE aspects and didn’t explicitly mention images, so Qwen3 may be primarily text. If needed, they could incorporate images since Qwen2.5 had it. [Hunyuan](https://openai.tencent.com/en/solutions/hunyuan) (Tencent’s model, not detailed here) also had a multimodal version. DeepSeek hasn’t described an image version publicly; they seem more focused on language and reasoning. Kimi K2 is interesting: it calls itself an “Agentic” model with tool use, but what about images? The Nature article refers to it as a chatbot, not necessarily multimodal. Possibly Kimi K2 is text (and code) only. However, because Kimi is aimed at being an AI assistant, it might use tools to handle images (like call an image captioning module), rather than having built-in image understanding.

**Training Data for Multimodality:** Models like Llama 4 used a mix of publicly available image-text data (e.g. web images with alt text, or curated datasets like COCO, Visual Genome, etc.) and possibly internal data (Meta has Instagram/Facebook images with user alt text or tags, though they’d have to be careful with privacy). They mention using “publicly shared posts from Instagram and Facebook” in training – many of those include images with accompanying text/captions, which could serve as multimodal pairs. This helps the model learn e.g. what a cat looks like when the text says “here is my cat Fluffy”.

**Implementation Notes:** In code, a simple way to do early fusion is: - Have a special token or sequence that represents an image (for example, some models prepend a ```<image>``` token, followed by a series of vector embeddings representing the image). - The image can be turned into embeddings by taking a pre-trained vision transformer (like ViT) and possibly fine-tuning it jointly with the language model. - During training, you intermix image-conditioned tasks with regular text tasks. For instance, one training sample might be: ```[Image] <|image|> A photo of two dogs playing in a park. Question: How many dogs are there? Answer: 2.``` – where ```<|image|>``` stands for the image patch sequence. - Over time, the model learns to associate that the image content matches “two dogs in a park” etc.

**Outputting Images or Other Modalities:** Most current multimodal LLMs only input images and output text (descriptions, answers). Some research models generate images given text (e.g. generative vision models like Imagen or [DALL-E](https://openai.com/dall-e)), but those are typically separate diffusion models or similar, not integrated in the LLM. Gemini and others are not (yet) generating images; they’re doing text about images. Audio input (like giving an audio file) is presumably transcribed internally or processed via an audio encoder, and the model outputs text or answers about it. Video input similarly might be processed to text output. So “multimodal” here usually means understanding multiple modalities and producing natural language responses.

**Use Cases Enabled:** With multimodal models, an AI assistant can do things like: describe what it sees through a camera, answer questions about a diagram or chart, help debug code by reading screenshots, etc. This greatly expands the usefulness of LLMs. It also raises new training considerations like: ensuring the model doesn’t leak details from images that might be sensitive, or aligning the model’s “sight” with factual descriptions and not hallucinations.

Given the focus of our report on training, the main takeaway is: to train a multimodal LLM: - One needs a merged dataset of text and other modalities. - Possibly pre-train the model on text-only first (since that usually requires the most data), then fine-tune on multimodal tasks (which might have comparatively less data). - Indeed, some pipelines do a two-phase: pure text pretraining, then a vision-language fine-tune (with a smaller learning rate on text weights and higher on newly added vision weights). - The model’s vocabulary might include special markers for images or special tokens (like ```[IMG1]``` etc.) representing image placeholders.

Llama 4’s release suggests that even open models can be made multimodal at high quality. It achieved a new level of multimodal intelligence as one paper put it. And Google/DeepMind’s push with Gemini confirms that top-tier models are expected to be multimodal by default.

As we move forward, we’ll look at fine-tuning and alignment – which is what turns these raw pre-trained models (whether unimodal or multimodal) into useful assistants that follow instructions and behave well. Multimodal models also undergo alignment fine-tuning (for example, OpenAI did a vision-alignment for GPT-4 to refuse inappropriate image requests, etc.). The next chapter covers these critical post-pretraining steps.

# Fine-Tuning and Alignment: From Pre-trained Model to Helpful Assistant

After the massive undertaking of pre-training an LLM on generic data, we typically end up with a model that is a good predictor of text in general, but not necessarily aligned with human intent or ready to perform specific tasks out-of-the-box. Fine-tuning is the process of taking this base model and further training it on narrower datasets to impart desired behaviors. Alignment refers to shaping the model’s outputs to be truthful, non-harmful, and following user instructions. In this chapter, we discuss various fine-tuning paradigms, including **supervised instruction tuning**, **Reinforcement Learning from Human Feedback (RLHF)** (and modern preference‑optimization alternatives like DPO/ORPO/GPO), safety fine-tuning, and recent techniques involving AI-generated feedback or distillation. We’ll also highlight how models like DeepSeek R1, OpenAI’s ChatGPT/GPT-4, and others implement these steps.

**Supervised Instruction Tuning:** One common approach is to fine-tune the model on example interactions that demonstrate ideal behavior. For instance, we can create (or gather) a dataset of prompts and high-quality responses (often written by human annotators or domain experts). By fine-tuning on this, the model learns to produce more helpful and explicit answers. This is how models like GPT-3.5 (text-davinci-002, etc.) and open-source ones like Alpaca or LLaMA-Adapter were created – taking a pre-trained model and training on instruction-response pairs. It’s essentially supervised learning: the model is given an input (say, “Explain how a combustion engine works in simple terms.”) and the desired output (“A combustion engine works by...”). The loss is computed on the output to align it with the ground truth answer.

All the models we discuss underwent some form of instruction tuning: 
- LLaMA 4 has both pre-trained and instruction-tuned versions. The model card explicitly lists “Pre-trained models” vs “Instruction tuned models”, with the latter intended for assistant-like chat. 
- Qwen3 had a concept of base models vs instruct models too (their blog mentions “post-trained” models like Qwen3-30B-A3B were released along with base ones). The post-training likely includes instruction fine-tuning. 
- Kimi K2 has a K2-Instruct variant which is “post-trained for general-purpose chat and agentic experiences”. This implies they fine-tuned the base to follow instructions and perhaps to use tools (agentic behavior). 
- DeepSeek-V3 after pretraining also did Supervised Fine-Tuning (SFT) and Reinforcement Learning stages. DeepSeek doesn’t detail their SFT data, but presumably they fine-tuned on curated instruction data and safety data. 
- DeepSeek-R1 pipeline specifically describes multiple stages: they had two RL stages and two SFT stages. They mention “two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities”. Likely one SFT provided initial chain-of-thought data (like exemplars of reasoning steps) and another provided general instruction-following demonstrations.

Supervised fine-tuning tends to make the model more eager to comply and produce direct answers rather than the open-ended or completion-style output of a base model. For instance, a base model might continue a prompt in an arbitrary way, whereas an instructed model will treat the prompt as a user request to be answered helpfully.

**Reinforcement Learning from Human Feedback (RLHF):** Perhaps the most famous alignment technique, RLHF involves training the model using a reward signal derived from human preferences. The typical pipeline (as used in InstructGPT and ChatGPT) is: 1. **Collect human preference data:** Have humans rank or score outputs from the model for various prompts. 2. **Train a reward model** $R(x,y)$ that, given a prompt $x$ and model output $y$, predicts a scalar reward matching human preferences (the model is trained to output higher scores for outputs humans preferred). 3. **Optimize the policy (the LLM) using RL**: Use an algorithm like Proximal Policy Optimization (PPO) to adjust the language model’s parameters to increase the reward model’s score, while also not drifting too far from the original model (KL-divergence penalty typically used to avoid the model going out-of-distribution).

In formula form, the objective might be: maximize $E_{y \sim \pi_\theta}[R(x,y)] - \beta \, D_{\text{KL}}(\pi_\theta \|\| \pi_{\text{initial}})$, where $\pi_\theta$ is the current policy (LLM), and $\beta$ is a weight on the KL term that keeps the new model close to the pre-trained one. The policy gradient for an output $y$ would be roughly proportional to $(R(x,y) - b) \nabla_\theta \log \pi_\theta(y\|x)$, where $b$ is a baseline (like a running mean reward or the reward model’s predicted expected value) to reduce variance. This is essentially the REINFORCE algorithm (with a baseline), which is a core of RL in these settings.

RLHF can fine-tune very nuanced behavior: it’s how models learn to prefer helpful answers, avoid rude or irrelevant replies, refuse certain requests politely, and so on, in alignment with human values. For example, if the base model sometimes gives a terse answer vs a detailed one, and humans prefer the detailed one, RLHF will push the policy to be more detailed.

**Preference‑Optimization Alternatives (DPO/ORPO/GPO and related):** To simplify or replace PPO‑style RL, many systems now use offline preference optimization on comparisons: **DPO (Direct Preference Optimization)** optimizes a closed‑form objective derived from the implicit reward, avoiding explicit reward‑model training and on‑policy RL. **ORPO (Odds‑Ratio Preference Optimization)** is a reference‑free monolithic objective that contrasts favored vs. disfavored outputs during SFT. **GPO (Generalized Preference Optimization)** unifies DPO/IPO/SLiC as special cases, clarifying regularization and design choices across PO losses. These methods often reach RLHF‑like quality with less engineering, so they are increasingly used alongside or instead of PPO‑based RL. 

**Applications in our context:** - OpenAI GPT-4/GPT-3.5: used RLHF extensively with human raters for alignment. By 2025, these closed models have undergone multiple rounds of RLHF plus additional safety training. - DeepSeek-R1 is notable because they did RL not from human feedback, but from a defined reasoning objective. They said “it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT”. Specifically, DeepSeek-R1-Zero was trained with RL from a reward that encouraged long chain-of-thought and self-consistency (they mention capabilities like self-verification emerged). The reward might have been something like solving puzzles correctly or matching known solutions, etc. Then DeepSeek-R1 (the final) incorporated some supervised “cold start” plus RL to refine. 
- DeepSeek-R1’s pipeline had two RL stages: one to discover improved reasoning patterns (likely optimizing some correctness or logical coherence measure), and another to align with human preferences. That second stage suggests they indeed did a form of RLHF or at least RL with a human-like reward (maybe from a model of human preferences or from desired outputs) to ensure the model’s style/tone is user-friendly. 
- Qwen3’s training pipeline included in the post-training: stage (2) “reasoning-based RL” and stage (4) “general RL”. Stage 2 presumably used rule-based rewards to enhance reasoning (like maybe rewarding the model for solving math problems correctly or following logical steps). Stage 4 applied RL on a mixture of tasks including instruction following, format compliance, and agent abilities. This is effectively RLHF if those tasks involve human-like judgments. They mention stage 4 was to “correct undesired behaviors” across more than 20 tasks, which sounds like alignment to me. Possibly they used an automated reward (like comparing to a reference or using evaluators) or had humans label some outputs. 
- Anthropic’s Claude (not explicitly in our given sources, but known in industry) used a variant called Constitutional AI – basically they let the model self-criticize and improve outputs using a set of principles as a “constitution”, minimizing direct human feedback. This is a trend where AI generates feedback to fine-tune itself (we see glimpses of that in e.g. DeepSeek R1 distillation too).

**Safety Fine-Tuning and Tone Adjustments:** Alignment is not only about following instructions, but also refusing inappropriate ones and maintaining a certain tone (not being toxic, not leaking private data, etc.). LLaMA 4’s team explicitly did a safety fine-tuning focusing on refusals and tone. They: - Collected prompts including borderline or adversarial queries. - Collected or generated good responses that either safely refuse or answer carefully. - Fine-tuned the model on these. They say for Llama 4 they “modified safety data responses to follow tone guidelines” and targeted removing “preachy or overly moralizing language”. Many models initially tend to give lecturing refusals (“I’m sorry, but I cannot do that because it’s not appropriate...”). Llama 4 made a point to sound more natural and not scold the user. - They also worked on refusal rate: “driving down model refusals to benign prompts” meaning the model should not refuse normal requests (false refusals). This is a balancing act: refuse what needs refusal (like requests for illicit content) but comply with harmless requests without unnecessary lecturing. Llama 3 had started this, and Llama 4 continued it. - Tone improvements: They ensured the model can use correct formatting (like using lists, tables when appropriate) and speak more conversationally. They even have a preset system prompt with detailed persona/behavior instructions for Llama 4, which we saw in the model card – essentially telling it “You are an expert conversationalist... You never use phrases that imply moral superiority... Do not refuse prompts about political and social issues, etc.”. This system prompt is a clever way to steer the fine-tuned model at inference time, but the model was likely trained with that style so it naturally follows those guidelines.

**Distillation and Smaller Models:** Another aspect of fine-tuning is knowledge distillation – compressing a large model’s behavior into a smaller model. DeepSeek did this extensively. They distilled DeepSeek-R1’s reasoning into six dense models ranging from 1.5B to 70B (based on Qwen2.5 or Llama3 bases). For example, DeepSeek-R1-Distill-Qwen-32B outperformed OpenAI’s smaller baseline on many benchmarks. They achieved this by using samples generated by DeepSeek-R1 to fine-tune the smaller models. The smaller models learned to mimic the larger model’s chain-of-thought and quality, giving surprisingly strong performance for their size. Distillation is valuable to produce more efficient models for deployment, and also as a way to share some capabilities of a closed big model with open community (if the big one is open or at least accessible to generate data).

Qwen3 also mentions something similar: their small 4B model matching a 72B’s performance was likely due to training techniques and maybe some teacher-student as well. They explicitly say Qwen3-4B rivals Qwen2.5-72B on some base tasks. Achieving that probably involved heavy distillation and data augmentation from bigger models.

**Agentic/Tool Use Fine-tuning:** A specialized fine-tuning for “agent” behaviors (which we will cover more next section) involves teaching the model how to call external tools. For example, in OpenAI’s function calling or in something like Toolformer, the model is fine-tuned on data where the correct response includes making an API call or using a calculator. Kimi K2’s “agentic intelligence” likely comes from fine-tuning on conversations where the model should output e.g. ```<tool>search("query")``` and then incorporate the result. Qwen3’s stage 4 RL included agent capabilities (they mention that explicitly), meaning they likely trained it to use tools (maybe using a framework where a tool’s output is given and the model must integrate it).

**Sample Code for RLHF-like loop:** While actual RL training code is complex, conceptually one could sketch:
```python
# Pseudocode for RLHF PPO loop (high-level)
for iteration in range(N):
    batch_prompts = sample_prompts()        # get a batch of query prompts
    # Get model outputs (current policy)
    outputs = [model.generate(prompt) for prompt in batch_prompts]
    # Compute reward for each output using the learned reward model
    rewards = [reward_model(prompt, output) for prompt, output in zip(batch_prompts, outputs)]
    # Compute policy loss: maximize reward (so minimize -reward) plus KL regularization
    loss = compute_ppo_loss(model, batch_prompts, outputs, rewards, ref_model=initial_model)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```
The ```compute_ppo_loss``` would involve computing the log probabilities of the generated outputs under both the current model and a reference model (often a copy of the model before RL, to measure how far we’ve moved) and then applying the PPO objective. The details of PPO (clipped surrogate objective, etc.) ensure stable updates.

**Outcomes of Fine-Tuning:** After alignment fine-tuning, the model’s style changes notably: - It follows instructions or questions directly, often in a conversational tone. - It usually says “Sure! Here’s what I found...” rather than continuing the prompt literally. - It refuses disallowed requests with a brief apology and statement of inability (or whatever style is tuned). - It formats answers more helpfully (e.g., if you ask for a list, it gives a bulleted list). - It tries to be correct and not hallucinate as much – though not perfect, alignment often includes some fact-checking training. Models like Qwen3 integrate a verification step from R1 to V3, meaning they distilled a chain-of-thought that verifies facts. That helps truthfulness.

From the sources: - Qwen3’s thinking mode presumably checks facts internally (like it can “fact-check itself similar to OpenAI’s o3” as TechCrunch noted). - DeepSeek R1’s self-verification means after generating a solution, it might use another forward pass to check if the solution is correct, and if not, adjust (this was emergent in R1-Zero RL training). - These are advanced alignment behaviors not purely from human feedback but from clever reward design.

**Continuous Improvement:** Alignment is an ongoing process. Often, after deployment, there’s a feedback loop: gather user feedback, identify failure cases, and fine-tune again. For example, OpenAI has done multiple GPT-4 refinements (the “GPT-4 (2023-08)” etc. updates). Similarly, Llama 4’s card says “future versions of tuned models may be released as we improve model behavior with community feedback.”. So they plan to iterate using feedback from users.

In conclusion, fine-tuning and alignment transform a raw LLM into a more polished AI assistant. It involves both supervised learning on curated data and reinforcement learning to directly optimize for human-desired outputs. The combination of these has proven critical – a model like GPT-4 wouldn’t be nearly as effective if it was just pre-trained; it’s the alignment that makes it reliable and user-friendly. The research community has open-sourced many of these alignment recipes, which is why we saw a proliferation of chatGPT-like models (Alpaca, Vicuna, etc.) in 2023 using the same principles on smaller scales.

Next, we will delve deeper into a specific aspect of fine-tuning for reasoning, namely the idea of training models to perform multi-step reasoning or “chain-of-thought” – which has been highlighted in models like DeepSeek R1 and Google’s Gemini 2.5 (the so-called “thinking” models). This deserves its own discussion given how central it is becoming in advanced LLM paradigms.

# Reasoning and Chain-of-Thought Training

One of the most intriguing advancements in LLM training is teaching models not just to produce direct answers, but to reason through problems step-by-step. This is often referred to as chain-of-thought (CoT) reasoning. By making their internal thought process explicit (at least during training or prompting), models can tackle more complex tasks like math word problems, logical reasoning puzzles, and multi-hop questions that would stump a “one-shot” answer approach. Here we discuss how chain-of-thought can be integrated into training, and recent projects that exemplify this: e.g., DeepSeek-R1’s long-form reasoning via RL, Google’s Gemini “thinking” models, and Alibaba’s Qwen3 hybrid mode.

**Chain-of-Thought Prompting vs Training:** Initially, chain-of-thought was used at inference: you prompt the model with “Let’s think step by step” and it often improves accuracy by forcing it to articulate intermediate steps. But now, techniques exist to actually train the model to generate these intermediate steps on its own. This can be done by supervising the model on solutions that include reasoning, or by reinforcement learning that rewards correct reasoning.

**Supervised CoT Fine-tuning:** One method is to assemble a dataset of problems with explained solutions. For example, the GSM8K math dataset has questions with step-by-step human solutions. Fine-tuning a model on such data will encourage it to produce a detailed solution (with reasoning) rather than just outputting the final answer. If the model at inference is allowed to output the reasoning, great. But sometimes the reasoning should be hidden (the user just wants the answer, not the whole reasoning process). There are strategies: the model can generate the reasoning internally (like to a scratchpad) and then give the final answer outwardly.

**Hidden vs Visible Reasoning:** Some frameworks allow the model to do a “scratchpad” hidden from the user. For instance, Self-Ask or Tree of Thoughts algorithms where the model generates thoughts and evaluates them. However, mainstream chat interfaces usually either show everything model says or nothing. Qwen3 came up with a clever approach: they have a special ```<think>``` tag to delineate thinking content. In their provided code, after generation, they split out the "thinking content" (the chain-of-thought) from the final "content" which is the answer. This suggests Qwen3’s model is trained to produce something like: “```<think> … reasoning here … </think>``` Therefore, the answer is 42.” The user only sees “Therefore, the answer is 42.”, but the model had generated the reasoning in between. The ```enable_thinking=True``` option triggers this mode. This is a fascinating way to integrate CoT at training time and give a choice at inference.

**Reinforcement Learning for Reasoning:** DeepSeek-R1 is a prime example where RL (without supervised pre-training on CoT) led to emergent reasoning. DeepSeek-R1-Zero was trained via RL directly on a base model to maximize some notion of reasoning quality. They report that “with RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors”. These included self-verification (the model checking its own answers), reflection, and generating long CoTs. However, R1-Zero also had issues (rambling, not well-structured outputs). So they introduced a “cold-start data before RL” to get DeepSeek-R1 (final). That likely means they did some supervised finetuning to stabilize output format and language quality (maybe giving it a basic instruction-following ability and preventing infinite loops). The result, DeepSeek-R1, achieved performance comparable to OpenAI’s reference model o1 in math, code, reasoning tasks. Moreover, they open-sourced not just R1, but also six dense models distilled from R1 that also performed exceptionally well on reasoning tasks.

One key element was that R1’s RL objective must have explicitly encouraged correctness of reasoning. Possibly they created automatic rewards like: for math problems, give reward 1 if the final answer is correct (or proportional to correctness), and a smaller reward for showing a valid reasoning path. Or they might have had the model verify intermediate steps with an external solver and reward consistency. The R1 paper (mentioned as arXiv 2501.12948) likely details this. The mention “the first open research to validate incentivizing reasoning purely through RL” is significant. It means we can potentially improve a model’s logical reasoning without expensive human annotation, by crafting an RL reward that captures logical consistency or accuracy.

**Hybrid Models – Two Modes (Think / Fast):** Alibaba’s Qwen3 introduced an explicit hybrid mode approach. The model can operate in Thinking Mode, taking more time to produce a chain-of-thought for complex problems, or in Non-Thinking Mode for quick answers on simple queries. They allow the user to control this or even insert commands like ```/think``` or ```/no_think``` in prompts to switch modes dynamically. Under the hood, the model in thinking mode likely does something like we described: produce hidden CoT. In non-thinking mode, it just answers directly (like a regular LLM). Qwen3’s training pipeline had a “thinking mode fusion” stage (stage 3). That stage integrated the two capabilities by fine-tuning on a combined dataset of CoT data and regular instruction data, which was “generated by the enhanced thinking model from stage 2”. Essentially, after they trained a good reasoning model (stage 2 RL), they had it generate instruction-following data with and without CoT to create a unified model that can do both. This clever pipeline allowed them to smoothly merge the behaviors.

They emphasize this thinking budget control – Qwen3’s performance scales with the amount of “thinking” allowed. In testing, they probably saw that if you let it think for, say, 8 thought steps vs 2, it does better on a hard task (with more compute cost). This is analogous to how humans allocate time: tough question -> think longer. Qwen3 user can decide that: allocate more time (and thus cost) for complex tasks.

**Gemini 2.5 – Reasoning Integration:** Google’s Gemini 2.5 is explicitly described as a “thinking model”, with the idea that it “reasons through its thoughts before responding”. They note they've tried chain-of-thought prompting and RL in the past, and now with 2.5 they “combine a significantly enhanced base model with improved post-training” to build thinking in. They also mention going forward all their models will have these thinking capabilities built-in. From the performance described: - Gemini 2.5 Pro tops the human preference leaderboard (LMArena). - It leads in math and science without test-time tricks like majority voting. - It scored 63.8% on agentic coding eval (SWE-Bench Verified) with a custom agent setup – that is more about agent use (next section), but reasoning helps there too (like deciding how to split a coding task into steps). - They gave an example of 2.5 Pro producing a video game code from a one-line prompt by reasoning through it.

To train this, Google likely used a mix of supervised CoT and RLHF. One clue: they released Gemini 2.0 Flash earlier as the first “thinking” model (the context suggests it had an explicit chain-of-thought output visible). For 2.5, they probably refined that. Also, an internal detail: “Multi Round Coherence Resolution (MRCR) evaluations” were mentioned. Possibly, that’s about the model keeping track of its reasoning across multiple turns or ensuring consistency.

**Self-Consistency and Voting:** A technique often used at inference (not training) is self-consistency, where the model generates multiple solutions and picks the most common answer (assuming if multiple independent reasoning attempts converge, it's likely correct). Some training could approximate this: e.g., train the model to generate multiple reasoning paths internally and verify. But more commonly, it’s an inference trick. However, if a model is aware of the idea of verifying answers, it can do something akin to this itself (DeepSeek R1’s self-verification is in that spirit).

**Mathematical Reasoning:** A subset of reasoning is mathematical calculation and symbolic reasoning. Models are poor at actual arithmetic with large numbers or precise algebra, but chain-of-thought helps them break a problem down. Yet sometimes you want the model to use a tool (like a calculator) for exactness. We'll talk about tools soon. But interestingly, training models on chain-of-thought for math helps them learn the algorithmic structure, even if they occasionally make arithmetic slips. Some efforts (like Google’s Minerva, 2022) specifically fine-tuned a model on tons of step-by-step math solutions and got SOTA on math benchmarks. The trend continues: Qwen3’s increased math performance is likely from training on synthetic math Q&A with workings.

**Limitations and Challenges:** Encouraging reasoning can sometimes lead models to over-explain even when not needed or to hallucinate reasoning. A model might produce a convincing but wrong chain-of-thought. Therefore, part of alignment is ensuring the chain-of-thought also is factual. Some approaches try to have the model “reflect” on whether each step is justified (like ask itself questions). R1’s reflection ability came through RL – presumably, the reward favored consistent reasoning that yields correct final answers, penalizing contradictions or errors.

There’s also the risk that a chain-of-thought can contain sensitive info or policy-violating content (since normally the final answer might be filtered but what about the reasoning text?). That’s why if we do show reasoning to users, it needs the same safety filtering. If it’s hidden (like Qwen’s think mode hidden from end-user), then it’s less a direct safety concern to user, but still needs to be correct to get the right answer.

In summary, training for reasoning has moved from an auxiliary trick to a central paradigm in cutting-edge LLMs. We see: - Use of supervised long-form solutions to instill reasoning patterns. - RL to actually push the model to use those patterns because it improves results (especially on benchmarks that measure reasoning or multi-step tasks). - Architectural and prompting innovations to allow models to “think” in a controllable way (like Qwen’s toggle, or maybe future models that dynamically decide how much thinking to do on the fly).

All this aims to address the key weakness of earlier LLMs: reasoning and complex problem solving. The results so far are promising – models like DeepSeek R1 and Gemini 2.5 can achieve near state-of-the-art on challenging reasoning benchmarks, and sometimes even surprise us with creative problem solving. The ongoing research likely will refine this further, possibly enabling internal planning that is even more structured (like logic circuits or external memory usage).

Now, having explored how models can reason internally, our final technical topic is how they can act externally – using tools, APIs, or multi-step workflows in the outside world (the so-called agentic behavior). This is about training models to not just solve in their head, but to interface with other systems to complete tasks. We already touched on it with “agentic coding” etc., but let’s dive deeper into agent-based training.

# Agent-Based Training and Tool Use

As large language models become more capable, there is a growing interest in enabling them to **interact with external tools and environments** – effectively turning them into agents that can perform multi-step tasks like browsing the web, executing code, querying databases, or controlling devices. Rather than treating the LLM as a static Q&A system, we can have it take actions (in the form of tool usage) to gather information or affect the world. This section covers how we train and design LLMs to be tool-using agents, referencing models like Kimi K2 (designed for agentic tasks), Gemini 2.5’s agentic coding abilities, and general techniques like API calling and multi-agent self-play.

**Why Tool Use?** Even the largest LLM has limitations in knowledge (bounded by its training cutoff) and in certain skills (like precise calculation or up-to-date info retrieval). By giving an LLM access to tools – e.g., a search engine, a calculator, a Python interpreter, a calendar, etc. – we can dramatically extend its capabilities. However, to use tools effectively, the model must be trained or fine-tuned to understand when and how to invoke them, and how to incorporate the results. This requires a special training regimen where the target outputs include not only natural language but also actions.

**Toolformer Approach (Supervised Learning of API Calls):** One method (proposed by Meta in early 2023) was Toolformer, which automatically labeled training data with potential API calls that could help prediction. For instance, if a sentence is “The population of France is [MASK]”, it might insert a call like ```<WikiSearch("population of France")>``` which returns a result, and then continue the text with the answer. Training on this teaches the model that when faced with certain queries, inserting a tool call yields better predictions. This is a form of supervised fine-tuning where the model’s output sequence includes special tokens for tool use. One can similarly create or use data for things like math: “Calculate 1234 * 5678” → model outputs ```<Calculator("1234*5678")> = 7006652```.

**Action Space and Format:** Typically, to integrate with tools, one defines a set of actions the model can output. In simplest terms, you can reserve some tokens or formats that mean: “Use Tool X with input Y”. For example: - ```<<SEARCH: "query">>``` could be a token sequence meaning search the web. - ```<<CALC: "expression">>``` meaning evaluate a math expression. - ```<<CODE_EXEC: ...>>``` meaning run some code.

During inference, when the model outputs such a token sequence, a system intercepts it, performs the tool operation, and then returns the result back to the model (often as a special input token). The model can then continue, now incorporating the tool result. Training data for this would show sequences where the model’s output includes those tokens and the subsequent text shows the result usage.

**Agentic Task Fine-tuning:** Kimi K2 is explicitly described as an “Open-Source Agentic Model”, “designed for tool use, reasoning, and autonomous problem-solving.”. This implies that in Kimi’s training (especially the post-training/instruct phase), they included scenarios requiring multi-step tool usage. For example, a training conversation might be: - User: "Who won the Best Actor Oscar in the year the Berlin Wall fell?" - Model: (internally thinks need to find year Berlin Wall fell -> that is 1989, then find Best Actor 1989) - Model might output a search action: ```<TOOL: search("Best Actor Oscar 1989")>```. - Then the training data would contain the tool’s output (from some knowledge base) e.g. "[ToolOutput] The Best Actor Oscar in 1989 was awarded to Daniel Day-Lewis." - Then model responds: "It was awarded to Daniel Day-Lewis in 1989."

Creating such data could be done manually or semi-automatically: - Hand-craft some examples. - Use existing datasets (like WebGPT from OpenAI, which had human demonstrations of using a browser). - Use synthetic generation by instructing an existing agentic model (like GPT-4 with plugins) to produce conversation + tool logs.

Given Kimi’s strong coding and agentic bent, its creators likely emphasize tasks like: - Code generation with an execution step to verify outputs (the evaluation table for Kimi shows “Agentic Coding” tasks where multiple attempts are allowed and Kimi scores well, though interestingly Claude with extended thinking is higher on multi-attempt agentic coding). - Multi-hop question answering using search. - Maybe interacting with a virtual environment (the mention "autonomous problem-solving" is broad; it could be marketing speak or could hint at some simulated environments or multi-agent tasks).

**Gemini 2.5 and Agentic Code:** The blog notes “visually compelling web apps and agentic code applications” and states on the SWE-Bench Verified (Agentic Coding) test, Gemini 2.5 Pro scores 63.8% with a custom agent setup. This suggests: - Agentic coding means writing code possibly with the ability to call external libraries or run and debug the code. Perhaps SWE-Bench is an evaluation where the model can attempt to solve a coding problem, run the code (via an agent interface), see if tests pass, and adjust if not. The multiple attempts metric in Kimi’s table aligns with that (like “Single Attempt vs Multiple Attempts” accuracy). - Scoring 63.8% with an agent means the model, when allowed to run code and fix errors, solved 63.8% of tasks. Without that, presumably its static pass@1 might be lower.

To train for this, one approach is **self-play or simulation**: have the model practice writing code and then actually executing it to see if it works, and learn from failures. This can be done via reinforcement learning or iterative prompting. Possibly, Google fine-tuned Gemini with data of the model interacting with a code execution environment.

**Multi-Agent Environments**: Some research explores multiple LLMs interacting (e.g., one as a user, one as an assistant, or a group cooperating on a task). Arena training by Baidu (2019, earlier concept) had agents debate and learn. More recent, there’s self-chat where a model plays both roles to generate dialogue data. It’s not clear if mainstream models explicitly did multi-agent training, but it’s a possibility for data generation. The Arena mentioned in Google’s blog is LMArena for eval, not to be confused with training.

**DeepSeek and Agentic Aspects**: TechCrunch mentions Qwen3 surpasses DeepSeek’s R1 on some benchmarks and also that Qwen3 was optimized for tool use. DeepSeek R1 itself mostly focused on reasoning, but their distilled models might incorporate general capabilities. There's a mention in TechCrunch: “Qwen3-32B surpasses OpenAI’s o1 on LiveCodeBench” and also “including Chinese AI lab DeepSeek’s R1” implying R1 had some agent or coding eval too.

**Memory and Autonomy**: Going beyond single tool calls, there’s interest in making LLM agents that remember goals and iterate (AutoGPT-like loops). Training directly for that is tough because it’s more of a runtime orchestration. Instead, the approach is often to hard-code the loop (with the model generating next action until done). But one could fine-tune a model to output an action, then some reflection, then next action – essentially one model managing the loop itself. This is advanced and still experimental; not sure if any 2025 model fully learned that internally or if they rely on external controllers.

**Safety in Tools**: When enabling tool use, alignment must extend to ensure the model doesn’t misuse tools. For example, if given a shell execution tool, it could potentially do harmful operations. Safety fine-tuning must incorporate rules like: disallow certain tool actions. Or implement a separate guardrail outside the model.

**Evaluation Benchmarks**: As seen with Kimi and Gemini, new benchmarks have emerged to measure agentic ability (SWE-bench agentic coding, Codeforces for code competition, etc.). A model that can use tools often vastly outperforms one that can’t on certain tasks – for example, searching knowledge vs relying on memory.

**Example (Pseudo)**: Suppose we want to fine-tune a model to use a Calculator and WebSearch: We might format training data as:
```
User: What is the capital of France and what's 25*79?
Assistant: Searching for capital of France...
<<SEARCH: "capital of France">>
[SearchResult] The capital of France is Paris.
Assistant: The capital of France is Paris. Now calculating 25*79...
<<CALC: "25*79">>
[CalcResult] 1975
Assistant: 25 * 79 = 1975. So, the answers are: Paris, and 1975.
```
Everything from "Assistant:" is the model’s output, including the special ```<<SEARCH>>``` and ```<<CALC>>``` actions. The ```[SearchResult]``` and ```[CalcResult]``` lines are the environment’s responses (which we feed back into the model’s input stream). We fine-tune the model on sequences like this (the entire conversation). Over time, it learns that this is the correct behavior: use search for knowledge, use calc for math.

At inference, we need an agent framework to actually carry out those actions when the tokens << >> appear. This moves beyond pure modeling into system design, but the model’s training is what makes it generate those tokens appropriately.

Kimi’s GitHub or huggingface might provide insight. Indeed, Kimi’s README (which we saw) lists it as agentic. And there's a comment in Nature: “K2 is not a 'reasoner' — not trained to approach queries step-by-step logic. Instead, it specializes in being an agentic LLM, carrying out multi-step tasks using tools like browsing or calling on math software.”. It even notes “some ChatGPT versions can do this but proprietary; researchers are checking if Kimi K2 can replicate examples of agentic behaviour Moonshot says K2 can do.”. So Kimi’s training claims it can, but people are verifying. This underscores how new this territory is.

**AutoGPT and such**: Those are frameworks where an LLM generates its own next prompt to itself etc. They weren’t exactly trained differently; they’re usage patterns. But a model like Kimi K2 might be more amenable to such autonomous looping because it was built with that in mind (maybe it is better at following a plan, or splitting tasks).

In summary, training agentic capabilities typically involves: - Supervised examples of tool usage within dialogues or text. - Reward techniques to encourage correct usage (like penalize if using a tool when not needed or not using when should). - Possibly multi-step environment interactions via RL (though that’s complex to do at scale, some research does it for specific tasks like code eval).

All the models discussed show that the frontier of LLMs is not just bigger models, but **more intelligent behavior**: reasoning well and acting in the world. Combining these yields powerful systems: e.g., a model that can reason (chain-of-thought) and then decide to use a tool if needed, then incorporate the result, then produce a final answer. We now have the pieces: huge knowledge (pre-training), alignment (to follow instructions safely), reasoning, and tool use. The state-of-the-art systems of 2025 integrate all of these.

# Conclusion and Outlook

In this report, we have journeyed from the first principles of language model training to the advanced paradigms that define **modern LLMs in 2025**. We began with the fundamental objective of next-token prediction and the Transformer architecture that underpins today’s models. From there, we saw how scaling up parameters and data – guided by scaling laws – led to ever more capable models, necessitating innovations like **mixture-of-experts** to reach trillions of parameters without prohibitive compute cost. We explored how context windows have expanded massively, with models like LLaMA 4 and Gemini 2.5 supporting contexts in the hundreds of thousands or more tokens, allowing them to digest extremely large inputs. We discussed the move to **multimodality**, enabling models to see images and hear audio, thus broadening their understanding of the world beyond text.

Crucially, we delved into the fine-tuning and alignment stage that transforms a raw model into a helpful assistant. Techniques like **supervised instruction tuning** and **RLHF** ensure models follow human instructions, stay polite, and output useful answers. We saw how Meta’s LLaMA 4, for instance, was fine-tuned to reduce unwanted refusals and adopt a more conversational tone. We also saw that open projects like DeepSeek R1 and Qwen3 have matched closed models by leveraging iterative fine-tuning pipelines (combining supervised CoT, RL for reasoning, and RLHF for general behavior).

Two especially cutting-edge paradigms stood out: **chain-of-thought reasoning** and **agentic tool use**. By incorporating chain-of-thought into training (either via supervised solutions or via RL incentives for reasoning), models like DeepSeek-R1 and Gemini 2.5 exhibit remarkable logical and analytical abilities, solving problems step-by-step that would have stumped earlier models. Meanwhile, by training models to use tools and act as agents, systems like Kimi K2 demonstrate how an LLM can go beyond text generation to actually execute code, call APIs, or perform multi-step tasks autonomously. These agentic capabilities are still maturing, but the groundwork has been laid through fine-tuning on tool-usage data and designing model outputs that interface with external systems.

The **implementation details** sprinkled through each chapter – from pseudocode of a Transformer training loop to examples of how a model might call a calculator – hopefully made these concepts concrete. We cited real numbers and findings from recent reports: for example, LLaMA 4’s training on 40T tokens and 7.38M GPU-hours, DeepSeek-V3’s use of FP8 and 2.8M H800 hours, Qwen3’s success with 36T tokens and a hybrid reasoning pipeline, and Kimi K2’s 1T-parameter MoE achieving state-of-the-art on several coding benchmarks. These concrete data points and citations demonstrate how theory translates into practice in current large-scale projects.

**Challenges and Future Directions**: Despite the impressive progress, there are challenges ahead: - **Efficiency**: Training these models is extremely expensive. Techniques like MoE, model compression, and more efficient algorithms will continue to be vital. Research into algorithmic efficiency (getting the same quality with less compute) is ongoing. - **Long-term coherence**: Even with long contexts, models can lose track of details or introduce contradictions in very long outputs. Methods to maintain consistency over long generations (perhaps via planning or state tracking) are needed. - **Reasoning limits**: Models still make logical mistakes, or they might overly rely on learned patterns that look like reasoning but aren’t robust. Combining symbolic logic or constraints with neural models is an area of exploration. - **Tool integration standardization**: Right now, each framework has its own way to do tools (OpenAI’s function calling, LangChain’s agent wrappers, etc.). In the future, model architectures might natively incorporate a form of API interaction (some researchers talk about neural-symbolic hybrids). - **Alignment and value lock-in**: As models get more autonomous (e.g., self-improving via agents), ensuring they remain aligned with human values is crucial. This might involve more complex multi-objective training (for helpfulness, truthfulness, harmlessness) and even involving multiple AIs in the training loop (one AI critiques or filters another, etc., as seen in Anthropic’s work). - **Personalization**: Training giant models that are one-size-fits-all is one approach, but there’s interest in fine-tuning models to users or domains. Techniques like prompt tuning, adapters, or on-device fine-tuning could allow customizing an LLM without retraining from scratch.

Given the rapid pace of research, what we consider “advanced” today (like 10M token context or tool use) might become standard practice, and new paradigms will emerge. Perhaps by 2026, we’ll talk about LLMs with integrated memory databases, or models that continuously learn online (current ones are static after training), or more likely the fusion of LLMs with other AI systems (vision, robotics etc.) for embodied agents.

In conclusion, modern LLM training is a beautiful synergy of **scalable engineering** and **scientific insight**: - From the engineering side, orchestrating thousands of GPUs, optimizing data pipelines, and implementing new architectures like MoE or retrieval-augmented models pushes the boundaries of what models can do. - From the science side, discoveries about how to prompt models, how they learn reasoning, and how to align them with human preference have transformed these systems from mere probability parrots to something akin to reasoning agents.

The critical works we referenced – LLaMA 4, Gemini 2.5, Qwen 3, DeepSeek V3/R1, Kimi K2 – represent the state-of-the-art culmination of these efforts in 2024-2025. They each contribute pieces to the puzzle: extreme scale, multimodality, reasoning, openness, agentic behavior, etc. By studying and integrating these advances, one can foresee an LLM of tomorrow that is: - As large as needed but efficient (maybe via MoE or sparse distributed representations), - Trained on not just internet text but multimodal, real-time data, - Capable of deep reasoning and planning, - Aligned with our values and able to safely act on our behalf (fetch info, automate tasks), - Perhaps even continually learning from interactions.

The path to such models is being paved by the principles and paradigms we discussed. It’s an exciting time in AI, where long-standing challenges are being tackled by creative combinations of ideas. For ML engineers and researchers, mastering these modern training techniques is key to pushing the field forward and building the next generation of intelligent systems.

**References**: 
  * [Meta AI. Llama 4 Scout and Maverick Model Card, HuggingFace.](https://huggingface.co/meta-llama)
  * [Meta AI. Llama 4: Community Blog Post, April 2025.](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)
  * [Google DeepMind. Gemini 2.5 Announcement, Mar 2025.](https://developers.googleblog.com/en/gemini-25-flash-lite-is-now-stable-and-generally-available/)
  * [Google DeepMind. Gemini 2.5 Blog, Mar 2025.](https://ai.google/)
  * [Alibaba DAMO. Qwen-3: Think Deeper, Act Faster (Blog), Apr 2025.](https://www.alizila.com/alibaba-unveils-new-qwen3-models-for-coding-complexing-reasoning-and-machine-translation/)
  * [TechCrunch. Alibaba unveils Qwen3, Apr 28 2025.](https://airevolution.poltextlab.com/alibaba-unveils-qwen3-open-source-ai-models-that-outperform-openais-o1/)
  * [DeepSeek AI. DeepSeek-V3 Model Card, Dec 2024.](https://huggingface.co/deepseek-ai/DeepSeek-V3)
  * [DeepSeek AI. DeepSeek-R1 Model Card, Jan 2025.](https://huggingface.co/collections/deepseek-ai/deepseek-r1-678e1e131c0169c0bc89728d)
  * [Moonshot AI. Kimi K2 GitHub README, Jul 2025.](https://github.com/MoonshotAI/Kimi-K2)
  * [Nature News. ‘Another DeepSeek moment’: Kimi K2 stirs excitement, Jul 16 2025.](https://ground.news/article/the-sequence-weekly-alpha-686-kimi-k2-is-a-trillion-parameter-open-source-model-you-must-know-about)
  * [F5 AI Blog. 10M Token Context Windows (Llama 4), Apr 2025.](https://www.f5.com/company/blog/rag-in-the-era-of-llms-with-10-million-token-context-windows)
  * [SSRN. Meta Llama 4: The Future of Multimodal AI, Apr 2025.](https://www.researchgate.net/publication/390687556_Meta_Llama_4_The_Future_of_Multimodal_AI)
  * [Qwen3: Think Deeper, Act Faster](https://www.c-sharpcorner.com/article/qwen3-think-deeper-act-faster-with-open-models/)
  * [Attention Is All You Need - Wikipedia](https://en.wikipedia.org/wiki/Attention_Is_All_You_Need)
  * [Gemini 2.5: Our newest Gemini model with thinking](https://economictimes.indiatimes.com/tech/technology/googles-ai-model-gemini-2-5-flash-to-support-local-processing-of-data/articleshow/122864621.cms)
  * [meta-llama/Llama-4-Scout-17B-16E · Hugging Face](https://huggingface.co/meta-llama)
  * [GitHub - MoonshotAI/Kimi-K2: Kimi K2 is the large language model series developed by Moonshot AI team](https://github.com/MoonshotAI/Kimi-K2)
  * [Alibaba unveils Qwen3, a family of 'hybrid' AI reasoning models](https://airevolution.poltextlab.com/alibaba-unveils-qwen3-open-source-ai-models-that-outperform-openais-o1/)
  * [All you need to know about Kimi K2: the Open-Weight, Agentic Titan Reshaping the AI Landscape](https://www.google.com/search?q=https://www.youtube.com/watch%3Fv%3D6AmqV_A32sE)
  * [‘Another DeepSeek moment’: Chinese AI model Kimi K2 stirs excitement](https://ground.news/article/the-sequence-weekly-alpha-686-kimi-k2-is-a-trillion-parameter-open-source-model-you-must-know-about)
  * [RAG in the Era of LLMs with 10 Million Token Context Windows](https://www.f5.com/company/blog/rag-in-the-era-of-llms-with-10-million-token-context-windows)
  * [Llama 4 10M context window : End of RAG (Retrieval-Augmented ...](https://www.f5.com/company/blog/rag-in-the-era-of-llms-with-10-million-token-context-windows)
  * [Llama 4 with 10M Tokens: How Much Does It Cost and Is It Worth It?](https://apidog.com/blog/llama-4-api/)
  * [deepseek-ai/DeepSeek-R1 · Hugging Face](https://huggingface.co/collections/deepseek-ai/deepseek-r1-678e1e131c0169c0bc89728d)
  * [Meta Llama 4: The Future of Multimodal AI by Ajit Singh :: SSRN](https://www.researchgate.net/publication/390687556_Meta_Llama_4_The_Future_of_Multimodal_AI)
  * [(PDF) Llama 4: Meta's Multimodal Leap in AI - ResearchGate](https://www.researchgate.net/publication/390581691_Llama_4_Meta's_Multimodal_Leap_in_AI_-Architecture_Capabilities_and_Comparative_Analysis)